{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from ncps and Keras\n",
    "from ncps import wirings\n",
    "from ncps.tf import LTC\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model for the LSTM\n",
    "lstm_model = keras.models.Sequential([\n",
    "    # Input layer that accepts sequences of variable length with 2 features\n",
    "    keras.layers.InputLayer(input_shape=(None, 2)),\n",
    "    \n",
    "    # Add an LSTM layer with 32 units\n",
    "    # Setting return_sequences=True means that the layer will output the full sequence\n",
    "    # This is useful for tasks where the next layer also expects sequences (e.g., another LSTM)\n",
    "    keras.layers.LSTM(units=32, return_sequences=True),  # Adjust units as needed\n",
    "])\n",
    "\n",
    "# Compile the LSTM model\n",
    "# The Adam optimizer is used with a learning rate of 0.01\n",
    "# Mean squared error loss is typically used for regression tasks\n",
    "lstm_model.compile(optimizer=keras.optimizers.Adam(0.01), loss='mean_squared_error')\n",
    "\n",
    "# Display the model's summary, which provides an overview of the architecture and parameters\n",
    "lstm_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Define the architecture for the Liquid Time-Constant Network (LTC)\n",
    "# Here, 'AutoNCP' creates a wiring configuration with 8 input units and 1 output unit\n",
    "ncp_arch = wirings.AutoNCP(8, 1)\n",
    "\n",
    "# Create a sequential model using Keras\n",
    "ncp_model = keras.models.Sequential(\n",
    "    [\n",
    "        # Input layer expects sequences of length None (variable length) with 2 features\n",
    "        keras.layers.InputLayer(input_shape=(None, 2)),\n",
    "        \n",
    "        # Add the LTC layer with the previously defined architecture\n",
    "        # The layer is set to return sequences, making it suitable for time series tasks\n",
    "        LTC(ncp_arch, return_sequences=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model with the Adam optimizer and mean squared error loss function\n",
    "ncp_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Learning rate set to 0.01\n",
    "    loss='mean_squared_error'  # Common loss function for regression tasks\n",
    ")\n",
    "\n",
    "# Display the model's summary, which provides an overview of the architecture and parameters\n",
    "ncp_model.summary()\n",
    "\n",
    "# Create a sequential model that combines the previously defined Liquid Neural Network (LNN) and LSTM models\n",
    "combined_model = keras.models.Sequential([\n",
    "    # Add the LNN model as the first layer in the combined model\n",
    "    ncp_model,\n",
    "    \n",
    "    # Add the LSTM model as the second layer in the combined model\n",
    "    lstm_model\n",
    "])\n",
    "\n",
    "# The combined model processes input data through the LNN first, followed by the LSTM\n",
    "# This structure allows for leveraging the capabilities of both models sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
